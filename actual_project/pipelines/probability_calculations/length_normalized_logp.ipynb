{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ],
   "id": "d850e36e96db253a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('/Users/mannes/thesis/correlation_evaluation/Qwen7B_math_run.csv')\n",
    "\n",
    "# df = pd.read_csv('/Users/mannes/thesis/complex_task_gen/output/output/df_eval_daniil.csv')\n",
    "df_corr = pd.read_csv('/Users/mannes/thesis/correlation_evaluation/correlation_table.csv')"
   ],
   "id": "1f41bdfd17a4307b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Assuming df_eval is your DataFrame\n",
    "alphas = np.linspace(-1, 2, 301)\n",
    "best_alpha, best_corr = None, -1\n",
    "\n",
    "for alpha in alphas:\n",
    "    df[\"score\"] = df[\"logp_sum\"] / (df[\"n_tokens\"] ** alpha)\n",
    "    rho, _ = spearmanr(df[\"score\"], df[\"pass1_int\"])  # assuming pass1_int is 1/0 pass label\n",
    "    if abs(rho) > best_corr:\n",
    "        best_alpha, best_corr = alpha, abs(rho)\n",
    "\n",
    "print(f\"Best alpha: {best_alpha:.3f}, correlation: {best_corr:.3f}\")"
   ],
   "id": "d3a43873f3980457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df[\"score_alpha\"] = df[\"logp_sum\"] / (df[\"n_tokens\"] ** -0.610)\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "spearman_corr = spearmanr(df[\"score_alpha\"], df[\"pass1_int\"])\n",
    "pearson_corr = pearsonr(df[\"score_alpha\"], df[\"pass1_int\"])\n",
    "\n",
    "print(\"Spearman:\", spearman_corr.correlation, \"p-value:\", spearman_corr.pvalue)\n",
    "print(\"Pearson:\", pearson_corr.correlation, \"p-value:\", pearson_corr.pvalue)\n"
   ],
   "id": "5f5d31cfb17a36a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "thresholds = np.linspace(df[\"score_alpha\"].min(), df[\"score_alpha\"].max(), 100)\n",
    "pass_rates = [(df[\"score_alpha\"] > t).mean() for t in thresholds]\n",
    "accuracies = [df[df[\"score_alpha\"] > t][\"pass1_int\"].mean() for t in thresholds]\n",
    "\n",
    "plt.plot(pass_rates, accuracies)\n",
    "plt.xlabel(\"Proportion of samples retained\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs coverage curve\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "id": "1eaac9a059ff5a24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b36dc2fb52b423f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, ttest_ind\n",
    "\n",
    "# Plot correlation vs alpha\n",
    "alphas = np.linspace(-1, 2, 301)\n",
    "correlations = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    df[\"score\"] = df[\"logp_sum\"] / (df[\"n_tokens\"] ** alpha)\n",
    "    rho, _ = spearmanr(df[\"score\"], df[\"pass1_int\"])\n",
    "    correlations.append(rho)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, correlations)\n",
    "plt.axvline(x=-0.610, color='r', linestyle='--', label=f'Optimal Î± = -0.610')\n",
    "plt.xlabel('Alpha value')\n",
    "plt.ylabel('Spearman correlation with pass@1')\n",
    "plt.title('Correlation vs Alpha Parameter')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate scores with optimal alpha\n",
    "df[\"score_optimal\"] = df[\"logp_sum\"] / (df[\"n_tokens\"] ** -0.610)\n",
    "\n",
    "# Group by correct/incorrect\n",
    "correct_scores = df[df[\"pass1_int\"] == 1][\"score_optimal\"]\n",
    "incorrect_scores = df[df[\"pass1_int\"] == 0][\"score_optimal\"]\n",
    "\n",
    "# Statistical summary\n",
    "summary = pd.DataFrame({\n",
    "    'Group': ['Correct', 'Incorrect'],\n",
    "    'Count': [len(correct_scores), len(incorrect_scores)],\n",
    "    'Mean': [correct_scores.mean(), incorrect_scores.mean()],\n",
    "    'Std': [correct_scores.std(), incorrect_scores.std()],\n",
    "    'Min': [correct_scores.min(), incorrect_scores.min()],\n",
    "    'Max': [correct_scores.max(), incorrect_scores.max()]\n",
    "})\n",
    "\n",
    "print(\"Summary statistics:\")\n",
    "print(summary)\n",
    "\n",
    "# Run t-test\n",
    "t_stat, p_value = ttest_ind(correct_scores, incorrect_scores, equal_var=False)\n",
    "print(f\"\\nT-test results: t-statistic = {t_stat:.4f}, p-value = {p_value:.8f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"The mean scores are statistically significantly different.\")\n",
    "\n",
    "# Visualize the distributions\n",
    "# Visualize the distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x=\"score_optimal\", hue=\"pass1_int\",\n",
    "             kde=True, common_norm=False, stat=\"density\",\n",
    "             palette=[\"green\", \"red\"])\n",
    "# Update legend labels after plotting\n",
    "plt.legend(title='Sample Status', labels=[\"Incorrect\", \"Correct\"])\n",
    "plt.title('Distribution of Scores for Correct vs Incorrect Samples')\n",
    "plt.xlabel('Score (logp_sum / (n_tokens ** -0.610))')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ],
   "id": "ccb7acaab7a9c498"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# df_corr = pd.read_csv('/Users/mannes/thesis/complex_task_gen/output/output/df_corr.csv')",
   "id": "56b19c9d0615a2ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Few ideas:\n",
    "# Try log of tokens\n",
    "# If we want for sanity check, try to run 7b model if possible -> Run entire pipeline with larger model\n",
    "\n",
    "# Try to run with 7b model -> This is the correlation between metric x and y\n",
    "# Then forget about this\n",
    "# Use Pass@1 as novelty -> Take target model, when I generate, I will use the pass@1 as novelty\n",
    "# Run model 4 times on -> calculate probability of the model being correct -> Then average this as the target model and give it as a reward -> more wasteful  but will still be good\n",
    "# For 1.5B it will be quick enoguh\n",
    "# Run target model on VLLM --> Generate 8 solutions for given task , average the  'likelihood' of the model being correct\n",
    "# Style of the solution can be"
   ],
   "id": "5bf6edb275294052"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_corr",
   "id": "28d26cc08036d2c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df",
   "id": "6a435e27c365b0e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80c33c8dffd47732"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run for 7B Model --> check that daniil is not insane\n",
    "# Update novelty score"
   ],
   "id": "67a21d49b608fc49"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
